{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Grad-CAM: Visualizing What the Model Sees\n",
    "\n",
    "## Overview\n",
    "\n",
    "Deep learning models are often called \"black boxes\". In this notebook, we'll use **Grad-CAM** (Gradient-weighted Class Activation Mapping) to understand what our models are looking at.\n",
    "\n",
    "### What is Grad-CAM?\n",
    "\n",
    "Grad-CAM produces a heatmap showing which parts of an image are most important for the model's prediction.\n",
    "\n",
    "**How it works:**\n",
    "1. Forward pass: Get feature maps from the last convolutional layer\n",
    "2. Backward pass: Get gradients of the target class score\n",
    "3. Weight feature maps by their gradient importance\n",
    "4. Create a heatmap highlighting important regions\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "| Question | Grad-CAM Helps Answer |\n",
    "|----------|----------------------|\n",
    "| Is the model cheating? | Check if it looks at faces or backgrounds |\n",
    "| Why did it misclassify? | See what features confused it |\n",
    "| What defines each emotion? | Which facial regions matter most |\n",
    "| Can we trust the model? | Verify it learns meaningful patterns |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "EMOTION_CLASSES = [\"anger\", \"disgust\", \"fear\", \"happiness\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "NUM_CLASSES = len(EMOTION_CLASSES)\n",
    "IDX_TO_EMOTION = {i: e for i, e in enumerate(EMOTION_CLASSES)}\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "with open('processed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "test_df = data['test_df'].reset_index(drop=True)\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model classes (same as training notebooks)\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(3, 32)\n",
    "        self.conv2 = ConvBlock(32, 64)\n",
    "        self.conv3 = ConvBlock(64, 128)\n",
    "        self.conv4 = ConvBlock(128, 256)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransferLearningModel(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(weights=None)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(nn.Dropout(0.5), nn.Linear(num_features, num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "print(\"Model classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load available models\n",
    "models_loaded = {}\n",
    "\n",
    "# Custom CNN\n",
    "try:\n",
    "    custom_cnn = CustomCNN()\n",
    "    checkpoint = torch.load('custom_cnn_best.pth', map_location=device, weights_only=False)\n",
    "    custom_cnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "    custom_cnn = custom_cnn.to(device)\n",
    "    custom_cnn.eval()\n",
    "    models_loaded['Custom CNN'] = custom_cnn\n",
    "    print(\"Loaded Custom CNN\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Custom CNN checkpoint not found - run notebook 03 first\")\n",
    "\n",
    "# Transfer Learning\n",
    "try:\n",
    "    transfer_model = TransferLearningModel()\n",
    "    checkpoint = torch.load('transfer_learning_best.pth', map_location=device, weights_only=False)\n",
    "    transfer_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    transfer_model = transfer_model.to(device)\n",
    "    transfer_model.eval()\n",
    "    models_loaded['ResNet18'] = transfer_model\n",
    "    print(\"Loaded ResNet18\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Transfer Learning checkpoint not found - run notebook 04 first\")\n",
    "\n",
    "print(f\"\\nLoaded {len(models_loaded)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Grad-CAM implementation for visualizing CNN attention.\n",
    "    \n",
    "    How it works:\n",
    "    1. Register hooks to capture feature maps and gradients\n",
    "    2. Forward pass to get prediction\n",
    "    3. Backward pass to get gradients of target class\n",
    "    4. Weight feature maps by gradient importance\n",
    "    5. Create heatmap showing important regions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, model_type='custom'):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        # Storage for hooks\n",
    "        self.feature_maps = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        # Register hooks on target layer\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"\n",
    "        Register forward and backward hooks on the target layer.\n",
    "        \n",
    "        Target layer is the last convolutional layer:\n",
    "        - Custom CNN: conv4\n",
    "        - ResNet18: layer4\n",
    "        \"\"\"\n",
    "        if self.model_type == 'custom':\n",
    "            target_layer = self.model.conv4\n",
    "        else:  # ResNet18\n",
    "            target_layer = self.model.backbone.layer4\n",
    "        \n",
    "        # Forward hook: save feature maps\n",
    "        def forward_hook(module, input, output):\n",
    "            self.feature_maps = output.detach()\n",
    "        \n",
    "        # Backward hook: save gradients\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "        \n",
    "        target_layer.register_forward_hook(forward_hook)\n",
    "        target_layer.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    def generate_heatmap(self, image_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        Generate Grad-CAM heatmap for an image.\n",
    "        \n",
    "        Args:\n",
    "            image_tensor: Preprocessed image tensor (1, 3, H, W)\n",
    "            target_class: Class to visualize (None = predicted class)\n",
    "            \n",
    "        Returns:\n",
    "            heatmap: 2D numpy array (H, W) with values 0-1\n",
    "            pred_class: Predicted class index\n",
    "            confidence: Softmax probability\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        output = self.model(image_tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        \n",
    "        # Get target class\n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        confidence = probs[0, target_class].item()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        output[0, target_class].backward()\n",
    "        \n",
    "        # Get gradients and feature maps\n",
    "        gradients = self.gradients  # (1, C, H, W)\n",
    "        feature_maps = self.feature_maps  # (1, C, H, W)\n",
    "        \n",
    "        # Global average pooling of gradients -> importance weights\n",
    "        weights = gradients.mean(dim=[2, 3], keepdim=True)  # (1, C, 1, 1)\n",
    "        \n",
    "        # Weighted combination of feature maps\n",
    "        cam = (weights * feature_maps).sum(dim=1, keepdim=True)  # (1, 1, H, W)\n",
    "        \n",
    "        # ReLU to keep only positive contributions\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # Resize to input size\n",
    "        cam = F.interpolate(cam, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Normalize to 0-1\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        \n",
    "        return cam, target_class, confidence\n",
    "\n",
    "print(\"GradCAM class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Helper Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(image_tensor):\n",
    "    \"\"\"\n",
    "    Convert normalized tensor back to displayable image.\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "    std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
    "    image = image_tensor.cpu() * std + mean\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "def overlay_heatmap(image, heatmap, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlay heatmap on image.\n",
    "    \n",
    "    Args:\n",
    "        image: RGB image (H, W, 3)\n",
    "        heatmap: 2D array (H, W) with values 0-1\n",
    "        alpha: Blending factor\n",
    "        \n",
    "    Returns:\n",
    "        Blended image\n",
    "    \"\"\"\n",
    "    # Apply colormap (jet: blue=low, red=high)\n",
    "    colormap = cm.jet(heatmap)[:, :, :3]\n",
    "    colormap = (colormap * 255).astype(np.uint8)\n",
    "    \n",
    "    # Blend\n",
    "    blended = (1 - alpha) * image + alpha * colormap\n",
    "    blended = blended.astype(np.uint8)\n",
    "    \n",
    "    return blended\n",
    "\n",
    "\n",
    "def load_and_preprocess(image_path):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    tensor = test_transform(image).unsqueeze(0).to(device)\n",
    "    return tensor\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Grad-CAM for Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradcam_samples(model, model_name, model_type, test_df, n_samples=8):\n",
    "    \"\"\"\n",
    "    Generate Grad-CAM visualizations for random test samples.\n",
    "    \"\"\"\n",
    "    gradcam = GradCAM(model, model_type)\n",
    "    \n",
    "    # Get random samples\n",
    "    samples = test_df.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 4, figsize=(14, 3*n_samples))\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows()):\n",
    "        # Load image\n",
    "        image_tensor = load_and_preprocess(row['image_path'])\n",
    "        true_label = row['label']\n",
    "        true_emotion = IDX_TO_EMOTION[true_label]\n",
    "        \n",
    "        # Generate Grad-CAM\n",
    "        heatmap, pred_class, confidence = gradcam.generate_heatmap(image_tensor)\n",
    "        pred_emotion = IDX_TO_EMOTION[pred_class]\n",
    "        \n",
    "        # Prepare display image\n",
    "        display_image = denormalize_image(image_tensor.squeeze())\n",
    "        overlay = overlay_heatmap(display_image, heatmap)\n",
    "        \n",
    "        # Determine if correct\n",
    "        correct = pred_class == true_label\n",
    "        color = 'green' if correct else 'red'\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(Image.open(row['image_path']))\n",
    "        axes[i, 0].set_title(f'Original\\nTrue: {true_emotion}', fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(display_image)\n",
    "        axes[i, 1].set_title('Preprocessed', fontsize=10)\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(heatmap, cmap='jet')\n",
    "        axes[i, 2].set_title('Grad-CAM Heatmap', fontsize=10)\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        axes[i, 3].imshow(overlay)\n",
    "        symbol = '✓' if correct else '✗'\n",
    "        axes[i, 3].set_title(f'Pred: {pred_emotion} {symbol}\\n({confidence:.1%})', \n",
    "                             fontsize=10, color=color)\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Grad-CAM Visualization: {model_name}', fontsize=14, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations for each model\n",
    "for model_name, model in models_loaded.items():\n",
    "    model_type = 'custom' if 'Custom' in model_name else 'resnet'\n",
    "    print(f\"\\nGenerating Grad-CAM for {model_name}...\")\n",
    "    visualize_gradcam_samples(model, model_name, model_type, test_df, n_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Grad-CAM Across Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_by_emotion(model, model_name, model_type, test_df):\n",
    "    \"\"\"\n",
    "    Show Grad-CAM for one correctly classified example of each emotion.\n",
    "    \"\"\"\n",
    "    gradcam = GradCAM(model, model_type)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(EMOTION_CLASSES), 3, figsize=(10, 3*len(EMOTION_CLASSES)))\n",
    "    \n",
    "    for i, emotion in enumerate(EMOTION_CLASSES):\n",
    "        # Find a sample of this emotion\n",
    "        emotion_samples = test_df[test_df['emotion'] == emotion]\n",
    "        \n",
    "        if len(emotion_samples) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Try to find a correctly classified sample\n",
    "        found = False\n",
    "        for _, row in emotion_samples.sample(frac=1, random_state=42).iterrows():\n",
    "            image_tensor = load_and_preprocess(row['image_path'])\n",
    "            heatmap, pred_class, confidence = gradcam.generate_heatmap(image_tensor)\n",
    "            \n",
    "            if pred_class == row['label']:\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            # Use first sample even if misclassified\n",
    "            row = emotion_samples.iloc[0]\n",
    "            image_tensor = load_and_preprocess(row['image_path'])\n",
    "            heatmap, pred_class, confidence = gradcam.generate_heatmap(image_tensor)\n",
    "        \n",
    "        # Display\n",
    "        display_image = denormalize_image(image_tensor.squeeze())\n",
    "        overlay = overlay_heatmap(display_image, heatmap)\n",
    "        \n",
    "        axes[i, 0].imshow(display_image)\n",
    "        axes[i, 0].set_ylabel(emotion.upper(), fontsize=12, rotation=0, ha='right', va='center')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(heatmap, cmap='jet')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(overlay)\n",
    "        axes[i, 2].set_title(f'{confidence:.1%}', fontsize=10)\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_title('Original', fontsize=11)\n",
    "    axes[0, 1].set_title('Heatmap', fontsize=11)\n",
    "    axes[0, 2].set_title('Overlay', fontsize=11)\n",
    "    \n",
    "    plt.suptitle(f'Grad-CAM by Emotion: {model_name}', fontsize=14, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate for each model\n",
    "for model_name, model in models_loaded.items():\n",
    "    model_type = 'custom' if 'Custom' in model_name else 'resnet'\n",
    "    print(f\"\\nGrad-CAM by emotion for {model_name}...\")\n",
    "    visualize_by_emotion(model, model_name, model_type, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_misclassifications(model, model_name, model_type, test_df, n_samples=6):\n",
    "    \"\"\"\n",
    "    Show Grad-CAM for misclassified examples.\n",
    "    \n",
    "    This helps understand WHY the model made mistakes.\n",
    "    \"\"\"\n",
    "    gradcam = GradCAM(model, model_type)\n",
    "    \n",
    "    misclassified = []\n",
    "    \n",
    "    # Find misclassified samples\n",
    "    for _, row in test_df.iterrows():\n",
    "        image_tensor = load_and_preprocess(row['image_path'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            pred_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        if pred_class != row['label']:\n",
    "            misclassified.append({\n",
    "                'image_path': row['image_path'],\n",
    "                'true_label': row['label'],\n",
    "                'pred_label': pred_class\n",
    "            })\n",
    "        \n",
    "        if len(misclassified) >= n_samples:\n",
    "            break\n",
    "    \n",
    "    if len(misclassified) == 0:\n",
    "        print(f\"No misclassifications found for {model_name}!\")\n",
    "        return\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(len(misclassified), 3, figsize=(10, 3*len(misclassified)))\n",
    "    \n",
    "    if len(misclassified) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, sample in enumerate(misclassified):\n",
    "        image_tensor = load_and_preprocess(sample['image_path'])\n",
    "        heatmap, _, confidence = gradcam.generate_heatmap(image_tensor, sample['pred_label'])\n",
    "        \n",
    "        display_image = denormalize_image(image_tensor.squeeze())\n",
    "        overlay = overlay_heatmap(display_image, heatmap)\n",
    "        \n",
    "        true_emotion = IDX_TO_EMOTION[sample['true_label']]\n",
    "        pred_emotion = IDX_TO_EMOTION[sample['pred_label']]\n",
    "        \n",
    "        axes[i][0].imshow(display_image)\n",
    "        axes[i][0].set_ylabel(f'True: {true_emotion}', fontsize=10)\n",
    "        axes[i][0].axis('off')\n",
    "        \n",
    "        axes[i][1].imshow(heatmap, cmap='jet')\n",
    "        axes[i][1].axis('off')\n",
    "        \n",
    "        axes[i][2].imshow(overlay)\n",
    "        axes[i][2].set_title(f'Pred: {pred_emotion}', fontsize=10, color='red')\n",
    "        axes[i][2].axis('off')\n",
    "    \n",
    "    axes[0][0].set_title('Original', fontsize=11)\n",
    "    axes[0][1].set_title('Attention', fontsize=11)\n",
    "    axes[0][2].set_title('Overlay', fontsize=11)\n",
    "    \n",
    "    plt.suptitle(f'Misclassification Analysis: {model_name}', fontsize=14, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze misclassifications\n",
    "for model_name, model in models_loaded.items():\n",
    "    model_type = 'custom' if 'Custom' in model_name else 'resnet'\n",
    "    print(f\"\\nAnalyzing misclassifications for {model_name}...\")\n",
    "    visualize_misclassifications(model, model_name, model_type, test_df, n_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Interpretation Guidelines\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "| Pattern | Interpretation |\n",
    "|---------|---------------|\n",
    "| **Focus on face** | Good! Model learned relevant features |\n",
    "| **Focus on background** | Bad! Model may be \"cheating\" |\n",
    "| **Focus on eyes/brows** | Common for anger, fear, surprise |\n",
    "| **Focus on mouth** | Common for happiness, disgust |\n",
    "| **Diffuse attention** | Model uncertain, features unclear |\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Background focus**: Model learned dataset bias, not faces\n",
    "2. **Wrong region**: Model looking at irrelevant features\n",
    "3. **Too localized**: Missing important facial cues\n",
    "4. **Too diffuse**: Model hasn't learned specific features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Grad-CAM reveals model attention** - Shows which image regions drive predictions\n",
    "\n",
    "2. **Validates model learning** - Confirms model focuses on faces, not backgrounds\n",
    "\n",
    "3. **Explains errors** - Shows why misclassifications happen\n",
    "\n",
    "4. **Emotion-specific patterns** - Different emotions activate different facial regions\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Good models** focus on facial features (eyes, mouth, brows)\n",
    "- **Misclassifications** often occur when attention is diffuse or wrong\n",
    "- **Transfer learning** models often have more focused attention\n",
    "- **Grad-CAM** is essential for understanding and trusting deep learning models\n",
    "\n",
    "### Project Complete!\n",
    "\n",
    "You've now built and analyzed a complete facial expression recognition system:\n",
    "1. Data exploration and preprocessing\n",
    "2. Baseline model (HOG + SVM)\n",
    "3. Custom CNN from scratch\n",
    "4. Transfer learning with ResNet18\n",
    "5. Comprehensive evaluation and comparison\n",
    "6. Model interpretability with Grad-CAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
