{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration and Preprocessing\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll explore the **Muxspace Facial Expression Dataset** and prepare it for training our emotion classification models.\n",
    "\n",
    "### What We'll Learn:\n",
    "1. How to load and inspect the dataset\n",
    "2. Understanding class distribution (are some emotions more common?)\n",
    "3. Visualizing sample images from each emotion class\n",
    "4. Data preprocessing steps (resizing, normalization)\n",
    "5. Data augmentation techniques to prevent overfitting\n",
    "6. Creating train/validation/test splits\n",
    "\n",
    "### Why This Matters:\n",
    "Before training any machine learning model, we MUST understand our data. Common issues like:\n",
    "- Class imbalance (too many \"neutral\" faces, too few \"fear\")\n",
    "- Noisy labels (mislabeled images)\n",
    "- Data quality issues (corrupted images, wrong formats)\n",
    "\n",
    "...can all be caught during exploration and will save you hours of debugging later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "We'll use:\n",
    "- **pandas**: For loading and manipulating the CSV labels file\n",
    "- **numpy**: For numerical operations on image arrays\n",
    "- **matplotlib/seaborn**: For creating visualizations\n",
    "- **PIL (Pillow)**: For loading and displaying images\n",
    "- **torch/torchvision**: For PyTorch data loading utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set style for nicer plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Paths and Constants\n",
    "\n",
    "We'll set up all the paths to our data and define the emotion classes we're working with.\n",
    "\n",
    "### The 7 Emotion Classes:\n",
    "1. **Anger** - Furrowed brows, tight lips\n",
    "2. **Disgust** - Wrinkled nose, raised upper lip\n",
    "3. **Fear** - Wide eyes, raised eyebrows\n",
    "4. **Happiness** - Smile, raised cheeks\n",
    "5. **Neutral** - Relaxed face, no strong expression\n",
    "6. **Sadness** - Drooping mouth corners, lowered brows\n",
    "7. **Surprise** - Raised eyebrows, open mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Get the project root (parent of notebook_version folder)\n",
    "NOTEBOOK_DIR = Path().absolute()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"facial_expressions-master\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "LEGEND_PATH = DATA_DIR / \"data\" / \"legend.csv\"\n",
    "\n",
    "# Verify paths exist\n",
    "print(\"Checking paths...\")\n",
    "print(f\"  Project root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data directory exists: {DATA_DIR.exists()}\")\n",
    "print(f\"  Images directory exists: {IMAGES_DIR.exists()}\")\n",
    "print(f\"  Legend file exists: {LEGEND_PATH.exists()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EMOTION CLASSES\n",
    "# =============================================================================\n",
    "\n",
    "# The 7 emotion classes we'll use\n",
    "EMOTION_CLASSES = [\n",
    "    \"anger\",\n",
    "    \"disgust\", \n",
    "    \"fear\",\n",
    "    \"happiness\",\n",
    "    \"neutral\",\n",
    "    \"sadness\",\n",
    "    \"surprise\",\n",
    "]\n",
    "\n",
    "# Create mappings between emotion names and numeric labels\n",
    "# Neural networks need numeric labels, not strings!\n",
    "EMOTION_TO_IDX = {emotion: idx for idx, emotion in enumerate(EMOTION_CLASSES)}\n",
    "IDX_TO_EMOTION = {idx: emotion for idx, emotion in enumerate(EMOTION_CLASSES)}\n",
    "\n",
    "print(f\"\\nNumber of emotion classes: {len(EMOTION_CLASSES)}\")\n",
    "print(f\"\\nEmotion to index mapping:\")\n",
    "for emotion, idx in EMOTION_TO_IDX.items():\n",
    "    print(f\"  {emotion}: {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Explore the Labels\n",
    "\n",
    "The dataset comes with a CSV file (`legend.csv`) that maps each image filename to its emotion label.\n",
    "\n",
    "Let's load it and see what we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the legend CSV file\n",
    "df_raw = pd.read_csv(LEGEND_PATH)\n",
    "\n",
    "# Display basic info\n",
    "print(\"=\" * 60)\n",
    "print(\"RAW DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal samples: {len(df_raw)}\")\n",
    "print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique emotion values\n",
    "print(\"Unique emotion labels in the raw data:\")\n",
    "print(df_raw['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Detected: Inconsistent Label Casing!\n",
    "\n",
    "Notice how we have both `happiness` and `HAPPINESS`? This is a common data quality issue.\n",
    "\n",
    "We need to normalize all labels to lowercase to treat them as the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "# Create a copy to avoid modifying the original\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Step 1: Normalize emotion labels to lowercase\n",
    "df['emotion'] = df['emotion'].str.lower().str.strip()\n",
    "\n",
    "print(\"After normalizing to lowercase:\")\n",
    "print(df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Filter to only include our 7 emotion classes\n",
    "# (This removes 'contempt' which has very few samples, and the header row 'emotion')\n",
    "\n",
    "print(f\"\\nBefore filtering: {len(df)} samples\")\n",
    "\n",
    "df = df[df['emotion'].isin(EMOTION_CLASSES)].copy()\n",
    "\n",
    "print(f\"After filtering to 7 classes: {len(df)} samples\")\n",
    "\n",
    "# Step 3: Create full image paths and verify they exist\n",
    "df['image_path'] = df['image'].apply(lambda x: IMAGES_DIR / x)\n",
    "df['exists'] = df['image_path'].apply(lambda x: x.exists())\n",
    "\n",
    "print(f\"\\nImages that exist: {df['exists'].sum()}\")\n",
    "print(f\"Images missing: {(~df['exists']).sum()}\")\n",
    "\n",
    "# Keep only existing images\n",
    "df = df[df['exists']].copy()\n",
    "\n",
    "# Step 4: Add numeric labels\n",
    "df['label'] = df['emotion'].map(EMOTION_TO_IDX)\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(df)} samples\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Class Distribution\n",
    "\n",
    "Class imbalance is a major concern in classification tasks. If 80% of our data is \"neutral\" faces, the model might just predict \"neutral\" for everything and still get 80% accuracy!\n",
    "\n",
    "Let's visualize the distribution of emotion classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples per class\n",
    "class_counts = df['emotion'].value_counts()\n",
    "\n",
    "# Create a nice bar plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors = sns.color_palette('husl', n_colors=len(EMOTION_CLASSES))\n",
    "bars = axes[0].bar(class_counts.index, class_counts.values, color=colors)\n",
    "axes[0].set_xlabel('Emotion', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Samples', fontsize=12)\n",
    "axes[0].set_title('Class Distribution in Dataset', fontsize=14)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                 str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('Class Distribution (Percentage)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"\\nSamples per class:\")\n",
    "for emotion in EMOTION_CLASSES:\n",
    "    count = class_counts.get(emotion, 0)\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"  {emotion:12s}: {count:5d} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nImbalance ratio (max/min): {class_counts.max() / class_counts.min():.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "1. **Major imbalance**: `neutral` and `happiness` dominate the dataset\n",
    "2. **Minority classes**: `fear`, `disgust`, `anger` have very few samples\n",
    "3. **Impact**: We'll need to use techniques like:\n",
    "   - Weighted loss function (penalize mistakes on minority classes more)\n",
    "   - Stratified sampling (maintain class ratios in train/val/test splits)\n",
    "   - Data augmentation (create more variations of minority class samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Sample Images\n",
    "\n",
    "Let's look at some actual images from each emotion class to get a feel for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples_per_class(df, n_samples=4):\n",
    "    \"\"\"\n",
    "    Display sample images from each emotion class.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with image paths and emotion labels\n",
    "        n_samples: Number of samples to show per class\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(EMOTION_CLASSES), n_samples, \n",
    "                             figsize=(3*n_samples, 3*len(EMOTION_CLASSES)))\n",
    "    \n",
    "    for row, emotion in enumerate(EMOTION_CLASSES):\n",
    "        # Get samples for this emotion\n",
    "        emotion_df = df[df['emotion'] == emotion]\n",
    "        samples = emotion_df.sample(n=min(n_samples, len(emotion_df)), random_state=42)\n",
    "        \n",
    "        for col, (_, sample) in enumerate(samples.iterrows()):\n",
    "            # Load and display image\n",
    "            img = Image.open(sample['image_path'])\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].axis('off')\n",
    "            \n",
    "            # Add emotion label on first column\n",
    "            if col == 0:\n",
    "                axes[row, col].set_ylabel(emotion.upper(), fontsize=12, \n",
    "                                          rotation=0, ha='right', va='center')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Emotion Class', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show samples\n",
    "show_samples_per_class(df, n_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Image Properties\n",
    "\n",
    "Before we can feed images to a neural network, we need to understand their properties:\n",
    "- Size (width x height)\n",
    "- Color channels (RGB vs grayscale)\n",
    "- File format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a sample of images\n",
    "sample_df = df.sample(n=min(500, len(df)), random_state=42)\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "modes = []\n",
    "\n",
    "for _, row in sample_df.iterrows():\n",
    "    img = Image.open(row['image_path'])\n",
    "    widths.append(img.width)\n",
    "    heights.append(img.height)\n",
    "    modes.append(img.mode)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IMAGE PROPERTIES (from 500 sample images)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nImage sizes:\")\n",
    "print(f\"  Width  - min: {min(widths)}, max: {max(widths)}, mean: {np.mean(widths):.0f}\")\n",
    "print(f\"  Height - min: {min(heights)}, max: {max(heights)}, mean: {np.mean(heights):.0f}\")\n",
    "\n",
    "print(f\"\\nColor modes:\")\n",
    "mode_counts = Counter(modes)\n",
    "for mode, count in mode_counts.items():\n",
    "    print(f\"  {mode}: {count} ({100*count/len(modes):.1f}%)\")\n",
    "\n",
    "# Visualize size distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(widths, bins=30, color='steelblue', edgecolor='white')\n",
    "axes[0].set_xlabel('Width (pixels)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Image Widths')\n",
    "axes[0].axvline(np.mean(widths), color='red', linestyle='--', label=f'Mean: {np.mean(widths):.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(heights, bins=30, color='darkorange', edgecolor='white')\n",
    "axes[1].set_xlabel('Height (pixels)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribution of Image Heights')\n",
    "axes[1].axvline(np.mean(heights), color='red', linestyle='--', label=f'Mean: {np.mean(heights):.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "\n",
    "1. **Variable sizes**: Images have different dimensions - we'll need to resize them all to a fixed size\n",
    "2. **Mostly RGB**: Most images are color (RGB), but some might be grayscale (L mode)\n",
    "3. **We'll standardize to**:\n",
    "   - Size: 224x224 (standard for transfer learning with ImageNet models)\n",
    "   - Mode: RGB (convert grayscale to RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Data Augmentation\n",
    "\n",
    "Data augmentation is a technique to artificially increase the size of our training dataset by applying random transformations to images. This helps:\n",
    "\n",
    "1. **Prevent overfitting** - Model sees more variations\n",
    "2. **Improve generalization** - Model learns to handle different conditions\n",
    "3. **Handle limited data** - Especially important for minority classes\n",
    "\n",
    "### Augmentations we'll use:\n",
    "\n",
    "| Augmentation | Why? |\n",
    "|--------------|------|\n",
    "| Horizontal Flip | Faces are roughly symmetric |\n",
    "| Rotation (±10°) | Account for head tilt |\n",
    "| Color Jitter | Simulate different lighting conditions |\n",
    "| Normalization | Required for pretrained models |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEFINE TRANSFORMS\n",
    "# =============================================================================\n",
    "\n",
    "# ImageNet normalization values (required for pretrained models)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "IMG_SIZE = 224  # Standard size for transfer learning\n",
    "\n",
    "# Training transforms (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),      # Resize to fixed size\n",
    "    transforms.RandomHorizontalFlip(p=0.5),       # 50% chance of flip\n",
    "    transforms.RandomRotation(degrees=10),        # Rotate up to 10 degrees\n",
    "    transforms.ColorJitter(                       # Random color adjustments\n",
    "        brightness=0.2,\n",
    "        contrast=0.2\n",
    "    ),\n",
    "    transforms.ToTensor(),                        # Convert to tensor [0, 1]\n",
    "    transforms.Normalize(                         # Normalize with ImageNet stats\n",
    "        mean=IMAGENET_MEAN,\n",
    "        std=IMAGENET_STD\n",
    "    )\n",
    "])\n",
    "\n",
    "# Test/Validation transforms (NO augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "print(\"Training transforms:\")\n",
    "print(train_transform)\n",
    "print(\"\\nTest transforms:\")\n",
    "print(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmentation effects\n",
    "def show_augmentation_examples(image_path, transform, n_examples=6):\n",
    "    \"\"\"\n",
    "    Show multiple augmented versions of the same image.\n",
    "    \"\"\"\n",
    "    original = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_examples//2 + 1, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Show original\n",
    "    axes[0].imshow(original)\n",
    "    axes[0].set_title('Original', fontsize=12)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show augmented versions\n",
    "    for i in range(1, n_examples + 1):\n",
    "        # Apply transform (need to undo normalization for display)\n",
    "        augmented = transform(original)\n",
    "        \n",
    "        # Denormalize for display\n",
    "        mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "        std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
    "        img_display = augmented * std + mean\n",
    "        img_display = img_display.permute(1, 2, 0).numpy()\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img_display)\n",
    "        axes[i].set_title(f'Augmented {i}', fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Data Augmentation Examples', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Pick a sample image\n",
    "sample_image_path = df.iloc[100]['image_path']\n",
    "show_augmentation_examples(sample_image_path, train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create PyTorch Dataset and DataLoaders\n",
    "\n",
    "PyTorch uses two key abstractions for data loading:\n",
    "\n",
    "1. **Dataset**: Defines how to access individual samples\n",
    "2. **DataLoader**: Handles batching, shuffling, and parallel loading\n",
    "\n",
    "We'll create a custom Dataset class for our facial expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FacialExpressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for facial expression images.\n",
    "    \n",
    "    This class:\n",
    "    - Loads images from disk on-demand (memory efficient)\n",
    "    - Applies transforms (augmentation, normalization)\n",
    "    - Returns (image_tensor, label) pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame with 'image_path' and 'label' columns\n",
    "            transform: Optional torchvision transforms to apply\n",
    "        \"\"\"\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample by index.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (image_tensor, label)\n",
    "        \"\"\"\n",
    "        # Get image path and label from dataframe\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        label = row['label']\n",
    "        \n",
    "        # Load image and convert to RGB (handles grayscale images)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"FacialExpressionDataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE TRAIN/VALIDATION/TEST SPLITS\n",
    "# =============================================================================\n",
    "\n",
    "# Split ratios\n",
    "TEST_SPLIT = 0.2   # 20% for testing\n",
    "VAL_SPLIT = 0.1    # 10% for validation (of original data)\n",
    "\n",
    "# First split: separate test set (20%)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=42,\n",
    "    stratify=df['label']  # IMPORTANT: Maintain class ratios!\n",
    ")\n",
    "\n",
    "# Second split: separate validation from training\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=VAL_SPLIT / (1 - TEST_SPLIT),  # Adjust ratio\n",
    "    random_state=42,\n",
    "    stratify=train_val_df['label']\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining set:   {len(train_df):,} samples ({100*len(train_df)/len(df):.1f}%)\")\n",
    "print(f\"Validation set: {len(val_df):,} samples ({100*len(val_df)/len(df):.1f}%)\")\n",
    "print(f\"Test set:       {len(test_df):,} samples ({100*len(test_df)/len(df):.1f}%)\")\n",
    "print(f\"\\nTotal:          {len(train_df) + len(val_df) + len(test_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify stratification worked (class ratios should be similar)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, (name, split_df) in zip(axes, [('Train', train_df), ('Validation', val_df), ('Test', test_df)]):\n",
    "    counts = split_df['emotion'].value_counts()\n",
    "    ax.bar(counts.index, counts.values, color=colors)\n",
    "    ax.set_title(f'{name} Set Distribution (n={len(split_df)})')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Class ratios are preserved across all splits (stratified sampling worked!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE DATASETS AND DATALOADERS\n",
    "# =============================================================================\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FacialExpressionDataset(train_df, transform=train_transform)\n",
    "val_dataset = FacialExpressionDataset(val_df, transform=test_transform)\n",
    "test_dataset = FacialExpressionDataset(test_df, transform=test_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,      # Shuffle training data each epoch\n",
    "    num_workers=0,     # Use 0 for Windows compatibility\n",
    "    pin_memory=True    # Faster GPU transfer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,     # No shuffle for validation\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,     # No shuffle for testing\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created!\")\n",
    "print(f\"\\nBatches per epoch:\")\n",
    "print(f\"  Training:   {len(train_loader)}\")\n",
    "print(f\"  Validation: {len(val_loader)}\")\n",
    "print(f\"  Test:       {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data loader\n",
    "print(\"Testing data loader...\")\n",
    "\n",
    "# Get one batch\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nBatch shape: {images.shape}\")\n",
    "print(f\"  - Batch size: {images.shape[0]}\")\n",
    "print(f\"  - Channels: {images.shape[1]} (RGB)\")\n",
    "print(f\"  - Height: {images.shape[2]}\")\n",
    "print(f\"  - Width: {images.shape[3]}\")\n",
    "print(f\"\\nLabels shape: {labels.shape}\")\n",
    "print(f\"Labels: {labels.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize a Training Batch\n",
    "\n",
    "Let's visualize a batch of images as they would be fed to the model (after transforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(images, labels, n_show=8):\n",
    "    \"\"\"\n",
    "    Visualize a batch of images with their labels.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, n_show//2, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Denormalize for display\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
    "    std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
    "    \n",
    "    for i in range(n_show):\n",
    "        img = images[i] * std + mean\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        label = labels[i].item()\n",
    "        emotion = IDX_TO_EMOTION[label]\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'{emotion} ({label})', fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Training Batch (After Augmentation)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a fresh batch and visualize\n",
    "images, labels = next(iter(train_loader))\n",
    "show_batch(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've:\n",
    "\n",
    "1. **Loaded the dataset** - 13,690 labeled facial expression images\n",
    "\n",
    "2. **Cleaned the data** - Normalized labels, filtered to 7 classes\n",
    "\n",
    "3. **Analyzed class distribution** - Found significant imbalance (neutral/happiness dominate)\n",
    "\n",
    "4. **Visualized samples** - Understood what each emotion looks like\n",
    "\n",
    "5. **Set up data augmentation** - Horizontal flip, rotation, color jitter\n",
    "\n",
    "6. **Created train/val/test splits** - 70%/10%/20% with stratification\n",
    "\n",
    "7. **Built PyTorch DataLoaders** - Ready for training!\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 2**: Build and train a baseline model (HOG + SVM)\n",
    "- **Notebook 3**: Build and train a custom CNN\n",
    "- **Notebook 4**: Use transfer learning with ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataframes for use in other notebooks\n",
    "import pickle\n",
    "\n",
    "processed_data = {\n",
    "    'train_df': train_df,\n",
    "    'val_df': val_df,\n",
    "    'test_df': test_df,\n",
    "    'emotion_classes': EMOTION_CLASSES,\n",
    "    'emotion_to_idx': EMOTION_TO_IDX,\n",
    "    'idx_to_emotion': IDX_TO_EMOTION\n",
    "}\n",
    "\n",
    "# Save to notebook_version folder\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(\"Processed data saved to processed_data.pkl\")\n",
    "print(\"\\nThis file will be used by subsequent notebooks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
