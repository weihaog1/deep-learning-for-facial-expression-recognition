{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation and Comparison\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll compare all our models and analyze their performance.\n",
    "\n",
    "### Models to Compare:\n",
    "1. **Baseline**: HOG features + SVM\n",
    "2. **Custom CNN**: Trained from scratch\n",
    "3. **Transfer Learning**: ResNet18 fine-tuned\n",
    "\n",
    "### Metrics We'll Analyze:\n",
    "- Overall accuracy\n",
    "- Per-class precision, recall, F1-score\n",
    "- Confusion matrices\n",
    "- Common misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION_CLASSES = [\"anger\", \"disgust\", \"fear\", \"happiness\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "\n",
    "# Load results from each model\n",
    "results = {}\n",
    "\n",
    "# Baseline\n",
    "try:\n",
    "    with open('baseline_results.pkl', 'rb') as f:\n",
    "        results['HOG + SVM'] = pickle.load(f)\n",
    "    print(\"Loaded baseline results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Baseline results not found - run notebook 02 first\")\n",
    "\n",
    "# Custom CNN\n",
    "try:\n",
    "    with open('custom_cnn_results.pkl', 'rb') as f:\n",
    "        results['Custom CNN'] = pickle.load(f)\n",
    "    print(\"Loaded Custom CNN results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Custom CNN results not found - run notebook 03 first\")\n",
    "\n",
    "# Transfer Learning\n",
    "try:\n",
    "    with open('transfer_learning_results.pkl', 'rb') as f:\n",
    "        results['ResNet18'] = pickle.load(f)\n",
    "    print(\"Loaded Transfer Learning results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Transfer Learning results not found - run notebook 04 first\")\n",
    "\n",
    "print(f\"\\nLoaded {len(results)} model results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compare Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract accuracies\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['test_accuracy'] for name in model_names]\n",
    "\n",
    "# Print comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL ACCURACY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Model':<25} {'Test Accuracy':>15}\")\n",
    "print(\"-\" * 40)\n",
    "for name, acc in zip(model_names, accuracies):\n",
    "    print(f\"{name:<25} {acc*100:>14.2f}%\")\n",
    "\n",
    "# Best model\n",
    "best_idx = np.argmax(accuracies)\n",
    "print(f\"\\nBest Model: {model_names[best_idx]} ({accuracies[best_idx]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['steelblue', 'darkorange', 'forestgreen'][:len(model_names)]\n",
    "bars = ax.bar(model_names, [acc * 100 for acc in accuracies], color=colors, edgecolor='white', linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{acc*100:.2f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 5),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Highlight best model\n",
    "bars[best_idx].set_edgecolor('gold')\n",
    "bars[best_idx].set_linewidth(4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Improvement Analysis\n",
    "\n",
    "Let's see how much each model improves over the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results) >= 2:\n",
    "    baseline_acc = results['HOG + SVM']['test_accuracy'] if 'HOG + SVM' in results else accuracies[0]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"IMPROVEMENT OVER BASELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n{'Model':<25} {'Accuracy':>12} {'Improvement':>15}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for name, acc in zip(model_names, accuracies):\n",
    "        improvement = (acc - baseline_acc) * 100\n",
    "        sign = \"+\" if improvement >= 0 else \"\"\n",
    "        print(f\"{name:<25} {acc*100:>11.2f}% {sign}{improvement:>14.2f}%\")\n",
    "else:\n",
    "    print(\"Need at least 2 models to compare improvements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Per-Class Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get per-class F1 scores for each model\n",
    "per_class_f1 = {}\n",
    "\n",
    "for name in model_names:\n",
    "    if 'per_class_f1' in results[name]:\n",
    "        per_class_f1[name] = results[name]['per_class_f1']\n",
    "    else:\n",
    "        # Calculate from confusion matrix if available\n",
    "        cm = np.array(results[name]['confusion_matrix'])\n",
    "        # This is approximate - actual F1 would need predictions\n",
    "        precision = np.diag(cm) / (cm.sum(axis=0) + 1e-10)\n",
    "        recall = np.diag(cm) / (cm.sum(axis=1) + 1e-10)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "        per_class_f1[name] = f1.tolist()\n",
    "\n",
    "if per_class_f1:\n",
    "    # Create comparison DataFrame\n",
    "    f1_df = pd.DataFrame(per_class_f1, index=EMOTION_CLASSES)\n",
    "    print(\"Per-Class F1 Scores:\")\n",
    "    print(f1_df.round(3))\n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_class_f1:\n",
    "    # Grouped bar chart for per-class comparison\n",
    "    x = np.arange(len(EMOTION_CLASSES))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    for i, (name, f1_scores) in enumerate(per_class_f1.items()):\n",
    "        offset = (i - len(per_class_f1)/2 + 0.5) * width\n",
    "        bars = ax.bar(x + offset, f1_scores, width, label=name, color=colors[i])\n",
    "    \n",
    "    ax.set_xlabel('Emotion Class', fontsize=12)\n",
    "    ax.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax.set_title('Per-Class F1 Score Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(EMOTION_CLASSES, rotation=45, ha='right')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Confusion Matrix Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices side by side\n",
    "n_models = len(results)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
    "\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (name, res) in zip(axes, results.items()):\n",
    "    cm = np.array(res['confusion_matrix'])\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.1%', cmap='Blues',\n",
    "                xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES,\n",
    "                ax=ax, cbar=False)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    acc = res['test_accuracy'] * 100\n",
    "    ax.set_title(f'{name}\\nAccuracy: {acc:.2f}%', fontsize=12)\n",
    "\n",
    "plt.suptitle('Confusion Matrix Comparison (Normalized)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Error Analysis\n",
    "\n",
    "Let's identify which emotion pairs are most commonly confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze common confusions for each model\n",
    "print(\"=\" * 60)\n",
    "print(\"COMMON MISCLASSIFICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, res in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    cm = np.array(res['confusion_matrix'])\n",
    "    \n",
    "    # Get off-diagonal elements (misclassifications)\n",
    "    confusions = []\n",
    "    for i in range(len(EMOTION_CLASSES)):\n",
    "        for j in range(len(EMOTION_CLASSES)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusions.append((EMOTION_CLASSES[i], EMOTION_CLASSES[j], cm[i, j]))\n",
    "    \n",
    "    # Sort by frequency\n",
    "    confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"  Top 5 confusions (True -> Predicted: Count):\")\n",
    "    for true_label, pred_label, count in confusions[:5]:\n",
    "        print(f\"    {true_label:12s} -> {pred_label:12s}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Hardest and Easiest Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify which classes are hardest/easiest across all models\n",
    "if per_class_f1:\n",
    "    # Average F1 across all models for each class\n",
    "    avg_f1 = {emotion: np.mean([per_class_f1[model][i] for model in model_names]) \n",
    "              for i, emotion in enumerate(EMOTION_CLASSES)}\n",
    "    \n",
    "    sorted_emotions = sorted(avg_f1.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"EMOTION DIFFICULTY RANKING (by average F1)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n{'Rank':<6} {'Emotion':<15} {'Avg F1':>10} {'Difficulty':>15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for rank, (emotion, f1) in enumerate(sorted_emotions, 1):\n",
    "        if f1 > 0.7:\n",
    "            difficulty = \"Easy\"\n",
    "        elif f1 > 0.5:\n",
    "            difficulty = \"Medium\"\n",
    "        else:\n",
    "            difficulty = \"Hard\"\n",
    "        print(f\"{rank:<6} {emotion:<15} {f1:>10.3f} {difficulty:>15}\")\n",
    "    \n",
    "    print(f\"\\nEasiest: {sorted_emotions[0][0]} (F1={sorted_emotions[0][1]:.3f})\")\n",
    "    print(f\"Hardest: {sorted_emotions[-1][0]} (F1={sorted_emotions[-1][1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Summary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary figure\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Accuracy comparison (top left)\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "bars = ax1.bar(model_names, [acc * 100 for acc in accuracies], color=colors[:len(model_names)])\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax1.annotate(f'{acc*100:.1f}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 xytext=(0, 3), textcoords='offset points', ha='center', fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.set_title('Overall Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Per-class F1 heatmap (top right)\n",
    "if per_class_f1:\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    f1_matrix = np.array([per_class_f1[m] for m in model_names])\n",
    "    sns.heatmap(f1_matrix, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                xticklabels=EMOTION_CLASSES, yticklabels=model_names,\n",
    "                ax=ax2, vmin=0, vmax=1)\n",
    "    ax2.set_title('Per-Class F1 Scores', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Class difficulty (bottom left)\n",
    "if per_class_f1:\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    avg_f1_values = [avg_f1[e] for e in EMOTION_CLASSES]\n",
    "    colors_by_difficulty = ['green' if f > 0.7 else 'orange' if f > 0.5 else 'red' for f in avg_f1_values]\n",
    "    bars = ax3.barh(EMOTION_CLASSES, avg_f1_values, color=colors_by_difficulty)\n",
    "    ax3.set_xlabel('Average F1 Score')\n",
    "    ax3.set_title('Emotion Difficulty (Avg across models)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlim(0, 1)\n",
    "\n",
    "# Key findings (bottom right)\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "ax4.axis('off')\n",
    "\n",
    "findings = f\"\"\"\n",
    "KEY FINDINGS\n",
    "{'='*40}\n",
    "\n",
    "Best Model: {model_names[best_idx]}\n",
    "Best Accuracy: {accuracies[best_idx]*100:.2f}%\n",
    "\n",
    "Improvement over baseline:\n",
    "\"\"\"\n",
    "if len(results) > 1:\n",
    "    baseline_acc = accuracies[0]\n",
    "    for name, acc in zip(model_names[1:], accuracies[1:]):\n",
    "        improvement = (acc - baseline_acc) * 100\n",
    "        findings += f\"  {name}: +{improvement:.1f}%\\n\"\n",
    "\n",
    "if per_class_f1:\n",
    "    findings += f\"\\nEasiest emotion: {sorted_emotions[0][0]}\\n\"\n",
    "    findings += f\"Hardest emotion: {sorted_emotions[-1][0]}\\n\"\n",
    "\n",
    "ax4.text(0.1, 0.9, findings, transform=ax4.transAxes, fontsize=12,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Facial Expression Recognition - Model Comparison Summary', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Transfer Learning wins**: Pretrained models consistently outperform training from scratch\n",
    "\n",
    "2. **Deep learning beats traditional ML**: Both CNNs outperformed HOG+SVM baseline\n",
    "\n",
    "3. **Class imbalance matters**: Rare emotions (fear, disgust) are harder to classify\n",
    "\n",
    "4. **Common confusions**: Similar emotions (e.g., neutral/sadness) often confused\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Use transfer learning** for production systems\n",
    "2. **Collect more data** for underrepresented classes\n",
    "3. **Consider class weights** or oversampling for imbalanced classes\n",
    "4. **Use Grad-CAM** (next notebook) to verify model focuses on faces\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 6**: Visualize what the models learn with Grad-CAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
