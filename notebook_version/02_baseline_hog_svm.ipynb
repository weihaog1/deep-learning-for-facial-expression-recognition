{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Baseline Model: HOG Features + SVM\n",
    "\n",
    "## Overview\n",
    "\n",
    "Before diving into deep learning, we'll build a **traditional machine learning baseline** using:\n",
    "\n",
    "1. **HOG (Histogram of Oriented Gradients)** - Hand-crafted feature extraction\n",
    "2. **SVM (Support Vector Machine)** - Classic classifier\n",
    "\n",
    "### Why Build a Baseline First?\n",
    "\n",
    "1. **Benchmark**: We need something to compare our fancy deep learning models against\n",
    "2. **Sanity check**: If deep learning doesn't beat HOG+SVM, something is wrong\n",
    "3. **Understanding**: Helps us appreciate what deep learning automates\n",
    "4. **Faster iteration**: HOG+SVM trains in minutes, not hours\n",
    "\n",
    "### What is HOG?\n",
    "\n",
    "HOG captures the **distribution of gradient directions** in local regions of an image:\n",
    "\n",
    "1. Divide the image into small cells (e.g., 8x8 pixels)\n",
    "2. Compute gradient direction and magnitude for each pixel\n",
    "3. Create a histogram of gradient directions for each cell\n",
    "4. Normalize across larger blocks for illumination invariance\n",
    "5. Concatenate all histograms into a single feature vector\n",
    "\n",
    "**Key insight**: Facial expressions involve muscle movements that create distinct edge/gradient patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Processed Data\n",
    "\n",
    "We'll use the data splits from Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from notebook 1\n",
    "try:\n",
    "    with open('processed_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    train_df = data['train_df']\n",
    "    val_df = data['val_df']\n",
    "    test_df = data['test_df']\n",
    "    EMOTION_CLASSES = data['emotion_classes']\n",
    "    EMOTION_TO_IDX = data['emotion_to_idx']\n",
    "    IDX_TO_EMOTION = data['idx_to_emotion']\n",
    "    \n",
    "    print(\"Data loaded from processed_data.pkl\")\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Please run Notebook 1 first to generate processed_data.pkl\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For baseline, we combine train and val sets (we'll use cross-validation instead)\n",
    "# This gives us more training data\n",
    "train_full_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "print(f\"Full training set: {len(train_full_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understanding HOG Feature Extraction\n",
    "\n",
    "Let's visualize what HOG features look like on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG parameters\n",
    "# These control how features are extracted\n",
    "\n",
    "HOG_PARAMS = {\n",
    "    'orientations': 9,         # Number of gradient direction bins (0-180 degrees)\n",
    "    'pixels_per_cell': (8, 8), # Cell size for local histograms\n",
    "    'cells_per_block': (2, 2), # Cells per normalization block\n",
    "    'block_norm': 'L2-Hys',    # Block normalization method\n",
    "    'visualize': True,         # Return visualization image\n",
    "    'feature_vector': True     # Return as 1D vector\n",
    "}\n",
    "\n",
    "# Image size for baseline (smaller = faster)\n",
    "BASELINE_IMG_SIZE = 64\n",
    "\n",
    "print(\"HOG Parameters:\")\n",
    "for key, value in HOG_PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, size=BASELINE_IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Load an image and preprocess it for HOG extraction.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load image\n",
    "    2. Convert to grayscale (HOG works on intensity gradients)\n",
    "    3. Resize to fixed size\n",
    "    4. Convert to numpy array\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        size: Target size (width = height)\n",
    "        \n",
    "    Returns:\n",
    "        Grayscale image as 2D numpy array\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    # L = 0.299*R + 0.587*G + 0.114*B (perceptual luminance)\n",
    "    img = img.convert('L')\n",
    "    \n",
    "    # Resize to fixed size\n",
    "    img = img.resize((size, size))\n",
    "    \n",
    "    # Convert to numpy array and normalize to [0, 1]\n",
    "    img_array = np.array(img) / 255.0\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# Test on a sample image\n",
    "sample_path = train_df.iloc[0]['image_path']\n",
    "sample_img = load_and_preprocess_image(sample_path)\n",
    "\n",
    "print(f\"Sample image shape: {sample_img.shape}\")\n",
    "print(f\"Value range: [{sample_img.min():.3f}, {sample_img.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HOG features\n",
    "def visualize_hog(image, title=\"\"):\n",
    "    \"\"\"\n",
    "    Extract and visualize HOG features for an image.\n",
    "    \"\"\"\n",
    "    # Extract HOG features with visualization\n",
    "    features, hog_image = hog(\n",
    "        image,\n",
    "        orientations=HOG_PARAMS['orientations'],\n",
    "        pixels_per_cell=HOG_PARAMS['pixels_per_cell'],\n",
    "        cells_per_block=HOG_PARAMS['cells_per_block'],\n",
    "        block_norm=HOG_PARAMS['block_norm'],\n",
    "        visualize=True,\n",
    "        feature_vector=True\n",
    "    )\n",
    "    \n",
    "    # Rescale HOG image for better visualization\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title('Original (Grayscale)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(hog_image_rescaled, cmap='gray')\n",
    "    axes[1].set_title('HOG Visualization')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Show feature vector histogram\n",
    "    axes[2].hist(features, bins=50, color='steelblue', edgecolor='white')\n",
    "    axes[2].set_xlabel('Feature Value')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    axes[2].set_title(f'Feature Distribution (n={len(features)})')\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=14, y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Visualize for sample image\n",
    "sample_features = visualize_hog(sample_img, title=\"HOG Feature Extraction\")\n",
    "print(f\"\\nFeature vector length: {len(sample_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare HOG features across different emotions\n",
    "fig, axes = plt.subplots(len(EMOTION_CLASSES), 2, figsize=(8, 3*len(EMOTION_CLASSES)))\n",
    "\n",
    "for i, emotion in enumerate(EMOTION_CLASSES):\n",
    "    # Get a sample image for this emotion\n",
    "    sample = train_df[train_df['emotion'] == emotion].iloc[0]\n",
    "    img = load_and_preprocess_image(sample['image_path'])\n",
    "    \n",
    "    # Extract HOG\n",
    "    features, hog_image = hog(\n",
    "        img,\n",
    "        orientations=HOG_PARAMS['orientations'],\n",
    "        pixels_per_cell=HOG_PARAMS['pixels_per_cell'],\n",
    "        cells_per_block=HOG_PARAMS['cells_per_block'],\n",
    "        block_norm=HOG_PARAMS['block_norm'],\n",
    "        visualize=True,\n",
    "        feature_vector=True\n",
    "    )\n",
    "    hog_image = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "    \n",
    "    # Plot\n",
    "    axes[i, 0].imshow(img, cmap='gray')\n",
    "    axes[i, 0].set_ylabel(emotion.upper(), fontsize=12, rotation=0, ha='right', va='center')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(hog_image, cmap='gray')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "axes[0, 0].set_title('Original', fontsize=12)\n",
    "axes[0, 1].set_title('HOG Features', fontsize=12)\n",
    "\n",
    "plt.suptitle('HOG Features by Emotion Class', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "HOG captures the gradient structure of faces:\n",
    "- **Eyes and eyebrows** create strong horizontal/vertical gradients\n",
    "- **Mouth** shape affects gradients in lower face\n",
    "- **Different expressions** produce different gradient patterns\n",
    "\n",
    "However, HOG is a **fixed** feature extractor - it can't learn what's most important for our specific task. Deep learning can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Features for All Images\n",
    "\n",
    "Now we'll extract HOG features from all training and test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image):\n",
    "    \"\"\"\n",
    "    Extract HOG features from a preprocessed grayscale image.\n",
    "    \n",
    "    Args:\n",
    "        image: 2D numpy array (grayscale image)\n",
    "        \n",
    "    Returns:\n",
    "        1D feature vector\n",
    "    \"\"\"\n",
    "    features = hog(\n",
    "        image,\n",
    "        orientations=HOG_PARAMS['orientations'],\n",
    "        pixels_per_cell=HOG_PARAMS['pixels_per_cell'],\n",
    "        cells_per_block=HOG_PARAMS['cells_per_block'],\n",
    "        block_norm=HOG_PARAMS['block_norm'],\n",
    "        visualize=False,\n",
    "        feature_vector=True\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_features_from_dataframe(df, desc=\"Extracting features\"):\n",
    "    \"\"\"\n",
    "    Extract HOG features from all images in a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'image_path' and 'label' columns\n",
    "        desc: Description for progress bar\n",
    "        \n",
    "    Returns:\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        y: Label vector (n_samples,)\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        # Load and preprocess image\n",
    "        img = load_and_preprocess_image(row['image_path'])\n",
    "        \n",
    "        # Extract HOG features\n",
    "        features = extract_hog_features(img)\n",
    "        \n",
    "        features_list.append(features)\n",
    "        labels_list.append(row['label'])\n",
    "    \n",
    "    return np.array(features_list), np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from training and test sets\n",
    "print(\"Extracting features from training set...\")\n",
    "X_train, y_train = extract_features_from_dataframe(train_full_df, \"Training set\")\n",
    "\n",
    "print(\"\\nExtracting features from test set...\")\n",
    "X_test, y_test = extract_features_from_dataframe(test_df, \"Test set\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"\\nFeature vector size: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Scaling\n",
    "\n",
    "SVM is sensitive to feature scales. Features with larger values can dominate the decision.\n",
    "\n",
    "We'll use **StandardScaler** to normalize features to zero mean and unit variance.\n",
    "\n",
    "**Important**: Fit the scaler on training data only, then apply to test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform test data (using training statistics!)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling complete!\")\n",
    "print(f\"\\nBefore scaling:\")\n",
    "print(f\"  Training mean: {X_train.mean():.4f}, std: {X_train.std():.4f}\")\n",
    "print(f\"\\nAfter scaling:\")\n",
    "print(f\"  Training mean: {X_train_scaled.mean():.4f}, std: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train SVM Classifier\n",
    "\n",
    "### About SVM:\n",
    "\n",
    "Support Vector Machine finds the optimal hyperplane that separates classes with maximum margin.\n",
    "\n",
    "**Key parameters:**\n",
    "- **kernel**: Transformation applied to features. RBF (Radial Basis Function) can handle non-linear boundaries.\n",
    "- **C**: Regularization parameter. Higher = less regularization, might overfit.\n",
    "- **gamma**: RBF kernel parameter. Higher = more complex boundaries.\n",
    "\n",
    "We'll use `class_weight='balanced'` to handle class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVM classifier\n",
    "svm_classifier = SVC(\n",
    "    kernel='rbf',           # Radial Basis Function kernel\n",
    "    C=10,                   # Regularization parameter\n",
    "    gamma='scale',          # 1 / (n_features * X.var())\n",
    "    class_weight='balanced', # Handle class imbalance\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"SVM Classifier Configuration:\")\n",
    "print(f\"  Kernel: {svm_classifier.kernel}\")\n",
    "print(f\"  C (regularization): {svm_classifier.C}\")\n",
    "print(f\"  Gamma: {svm_classifier.gamma}\")\n",
    "print(f\"  Class weight: {svm_classifier.class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "print(\"Training SVM classifier...\")\n",
    "print(\"(This may take a few minutes)\\n\")\n",
    "\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Number of support vectors: {svm_classifier.n_support_.sum()}\")\n",
    "print(f\"Support vectors per class: {dict(zip(EMOTION_CLASSES, svm_classifier.n_support_))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Model\n",
    "\n",
    "Let's see how well our baseline performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = svm_classifier.predict(X_train_scaled)\n",
    "y_test_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Accuracy:     {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Check for overfitting\n",
    "gap = train_accuracy - test_accuracy\n",
    "print(f\"\\nGeneralization gap: {gap:.4f}\")\n",
    "if gap > 0.1:\n",
    "    print(\"Warning: Model may be overfitting (large gap between train and test)\")\n",
    "else:\n",
    "    print(\"Good: Model generalizes well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT (Test Set)\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, target_names=EMOTION_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES,\n",
    "            ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1%', cmap='Blues',\n",
    "            xticklabels=EMOTION_CLASSES, yticklabels=EMOTION_CLASSES,\n",
    "            ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.suptitle('HOG + SVM Baseline Model', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance visualization\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, y_test_pred, average=None\n",
    ")\n",
    "\n",
    "# Create bar chart\n",
    "x = np.arange(len(EMOTION_CLASSES))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "bars1 = ax.bar(x - width, precision, width, label='Precision', color='steelblue')\n",
    "bars2 = ax.bar(x, recall, width, label='Recall', color='darkorange')\n",
    "bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='forestgreen')\n",
    "\n",
    "ax.set_xlabel('Emotion Class')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Per-Class Performance (HOG + SVM Baseline)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(EMOTION_CLASSES, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Error Analysis\n",
    "\n",
    "Let's look at some misclassified examples to understand where the model struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples\n",
    "misclassified_idx = np.where(y_test != y_test_pred)[0]\n",
    "\n",
    "print(f\"Total misclassified: {len(misclassified_idx)} out of {len(y_test)} ({100*len(misclassified_idx)/len(y_test):.1f}%)\")\n",
    "\n",
    "# Analyze common mistakes\n",
    "mistakes = []\n",
    "for idx in misclassified_idx:\n",
    "    true_label = IDX_TO_EMOTION[y_test[idx]]\n",
    "    pred_label = IDX_TO_EMOTION[y_test_pred[idx]]\n",
    "    mistakes.append((true_label, pred_label))\n",
    "\n",
    "mistake_counts = Counter(mistakes)\n",
    "\n",
    "print(\"\\nMost Common Mistakes (True -> Predicted):\")\n",
    "for (true_label, pred_label), count in mistake_counts.most_common(10):\n",
    "    print(f\"  {true_label:12s} -> {pred_label:12s}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some misclassified examples\n",
    "test_df_reset = test_df.reset_index(drop=True)\n",
    "\n",
    "n_show = min(8, len(misclassified_idx))\n",
    "sample_misclassified = np.random.choice(misclassified_idx, n_show, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, n_show//2, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(sample_misclassified):\n",
    "    # Load original image\n",
    "    img = Image.open(test_df_reset.iloc[idx]['image_path'])\n",
    "    \n",
    "    true_label = IDX_TO_EMOTION[y_test[idx]]\n",
    "    pred_label = IDX_TO_EMOTION[y_test_pred[idx]]\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', \n",
    "                      fontsize=10, color='red')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Misclassified Examples', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Cross-Validation (Optional)\n",
    "\n",
    "For a more robust estimate of performance, let's use k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "print(\"Running 5-fold cross-validation...\")\n",
    "print(\"(This may take several minutes)\\n\")\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', random_state=42),\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(f\"  Fold scores: {cv_scores}\")\n",
    "print(f\"  Mean accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save the Model\n",
    "\n",
    "Let's save our trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and scaler\n",
    "baseline_model = {\n",
    "    'classifier': svm_classifier,\n",
    "    'scaler': scaler,\n",
    "    'hog_params': HOG_PARAMS,\n",
    "    'img_size': BASELINE_IMG_SIZE,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'emotion_classes': EMOTION_CLASSES\n",
    "}\n",
    "\n",
    "with open('baseline_model.pkl', 'wb') as f:\n",
    "    pickle.dump(baseline_model, f)\n",
    "\n",
    "print(\"Model saved to baseline_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built:\n",
    "- **Feature Extractor**: HOG (Histogram of Oriented Gradients)\n",
    "- **Classifier**: SVM with RBF kernel\n",
    "\n",
    "### Results:\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Training Accuracy | ~XX% |\n",
    "| Test Accuracy | ~XX% |\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **HOG captures gradient structure** but can't learn task-specific features\n",
    "2. **Some emotions are harder** (fear, disgust often confused with others)\n",
    "3. **Class imbalance hurts** minority class performance\n",
    "4. **This is our baseline** - deep learning should beat this!\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 3**: Build a custom CNN that learns features automatically\n",
    "- **Notebook 4**: Use transfer learning for even better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for comparison in later notebooks\n",
    "baseline_results = {\n",
    "    'model_name': 'HOG + SVM',\n",
    "    'train_accuracy': train_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'cv_scores': cv_scores.tolist() if 'cv_scores' in dir() else None,\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'per_class_f1': f1.tolist()\n",
    "}\n",
    "\n",
    "with open('baseline_results.pkl', 'wb') as f:\n",
    "    pickle.dump(baseline_results, f)\n",
    "\n",
    "print(\"Results saved for comparison!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
